{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\musta\\OneDrive\\Desktop\\tweet_emotions.csv\\tweet_emotions.csv')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters, URLs, and numbers\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # Keep only letters\n",
    "    # Tokenize and remove stopwords\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # Lemmatize words\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    # Rejoin the words into a single string\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_content'] = df['content'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Apply Bag of Words (BoW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df['cleaned_content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Train-Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, df['sentiment'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Train a Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Evaluate the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF - IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `cleaned_content` is your preprocessed text and `sentiment` is the target column\n",
    "X = df['cleaned_content']\n",
    "y = df['sentiment']\n",
    "\n",
    "# Apply TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Adjust max_features as needed\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset preview:\n",
      "     tweet_id   sentiment                                            content\n",
      "0  1956967341       empty  @tiffanylue i know  i was listenin to bad habi...\n",
      "1  1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
      "2  1956967696     sadness                Funeral ceremony...gloomy friday...\n",
      "3  1956967789  enthusiasm               wants to hang out with friends SOON!\n",
      "4  1956968416     neutral  @dannycastillo We want to trade with someone w...\n"
     ]
    }
   ],
   "source": [
    "file_path = r'C:\\Users\\musta\\OneDrive\\Desktop\\tweet_emotions.csv\\tweet_emotions.csv'  # Replace with your file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display dataset\n",
    "print(\"Dataset preview:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\musta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\musta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\musta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\musta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\musta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Remove special characters and convert to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and apply lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply preprocessing to the content column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to the content column\n",
    "df['cleaned_content'] = df['content'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare sentences for Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df['cleaned_content'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display a sample of tokenized sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample tokenized sentences:\n",
      "[['tiffanylue', 'know', 'listenin', 'bad', 'habit', 'earlier', 'started', 'freakin', 'part'], ['layin', 'n', 'bed', 'headache', 'ughhhhwaitin', 'call'], ['funeral', 'ceremonygloomy', 'friday'], ['want', 'hang', 'friend', 'soon'], ['dannycastillo', 'want', 'trade', 'someone', 'houston', 'ticket', 'one']]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSample tokenized sentences:\")\n",
    "print(sentences[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Train Word2Vec - CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training CBOW Word2Vec model...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTraining CBOW Word2Vec model...\")\n",
    "cbow_model = Word2Vec(sentences=sentences, vector_size=300, window=5, min_count=2, sg=0, epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save CBOW model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW model saved as 'word2vec_cbow.model'.\n"
     ]
    }
   ],
   "source": [
    "cbow_model.save('word2vec_cbow.model')\n",
    "print(\"CBOW model saved as 'word2vec_cbow.model'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Train Word2Vec - Skip-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Skip-Gram Word2Vec model...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTraining Skip-Gram Word2Vec model...\")\n",
    "skipgram_model = Word2Vec(sentences=sentences, vector_size=300, window=5, min_count=2, sg=1, epochs=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Skip-Gram model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip-Gram model saved as 'word2vec_skipgram.model'.\n"
     ]
    }
   ],
   "source": [
    "skipgram_model.save('word2vec_skipgram.model')\n",
    "print(\"Skip-Gram model saved as 'word2vec_skipgram.model'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Inspect Trained Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load saved models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_cbow = Word2Vec.load('word2vec_cbow.model')\n",
    "loaded_skipgram = Word2Vec.load('word2vec_skipgram.model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check similar words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Words similar to 'happy' using CBOW:\n",
      "[('bed', 0.8746517896652222), ('work', 0.8586323857307434), ('asleep', 0.8485585451126099), ('wake', 0.8461429476737976), ('takn', 0.8290516138076782), ('stay', 0.8240368962287903), ('pm', 0.819888710975647), ('bex', 0.8160650134086609), ('alone', 0.8144768476486206), ('tmrw', 0.8131993412971497)]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nWords similar to 'sleep' using CBOW:\")\n",
    "print(loaded_cbow.wv.most_similar('sleep', topn=10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Words similar to 'happy' using Skip-Gram:\n",
      "[('wide', 0.7662756443023682), ('woke', 0.7578635215759277), ('alarm', 0.7574796080589294), ('migraine', 0.7349479794502258), ('morn', 0.7315568327903748), ('awake', 0.7260857820510864), ('bed', 0.7243674993515015), ('ache', 0.7242431044578552), ('sleeping', 0.7222613096237183), ('nighty', 0.7218272089958191)]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nWords similar to 'sleep' using Skip-Gram:\")\n",
    "print(loaded_skipgram.wv.most_similar('sleep', topn=10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect word vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vector for 'sleep' using CBOW:\n",
      "[ 1.20704107e-01  7.64020443e-01 -6.73619688e-01 -4.14599568e-01\n",
      "  2.29132157e-02 -9.09570932e-01  6.18406773e-01  1.16969275e+00\n",
      " -6.28199458e-01  3.27487588e-01  5.96928596e-02 -2.62963295e-01\n",
      " -7.19414711e-01  2.93196261e-01  3.86416644e-01 -5.98978400e-01\n",
      "  5.41918874e-01 -6.65719688e-01 -2.04737946e-01 -6.32874250e-01\n",
      "  5.37055194e-01  9.27321017e-01  2.42641672e-01 -1.10293292e-02\n",
      "  5.21759212e-01 -4.43845958e-01 -2.17715241e-02  7.10286021e-01\n",
      " -7.15177476e-01 -8.32669258e-01 -2.45985270e-01  2.76036173e-01\n",
      " -8.55580196e-02  5.21364927e-01 -6.93235457e-01  1.58437997e-01\n",
      "  5.47635257e-01  8.65095407e-02 -1.83078691e-01  4.29074131e-02\n",
      " -4.02789235e-01 -1.02163649e+00 -8.46612155e-02 -1.33918971e-01\n",
      "  4.52159137e-01  3.41907650e-01 -1.03643787e+00 -1.64258465e-01\n",
      "  7.41261423e-01  3.73490788e-02 -1.97735086e-01 -2.69606560e-01\n",
      " -1.39355823e-01  3.33170772e-01  2.66192015e-02  1.43694758e-01\n",
      "  5.34660071e-02 -8.36372852e-01  6.47294700e-01 -1.34069994e-01\n",
      " -2.59406179e-01 -3.61502916e-01 -9.72869158e-01  5.13125300e-01\n",
      " -2.84952819e-01  5.95954716e-01 -2.08399087e-01  2.58155972e-01\n",
      " -8.85994196e-01 -3.55270267e-01  4.72541630e-01  1.11899315e-03\n",
      "  6.14126801e-01 -1.28656077e+00 -1.40862495e-01 -7.60713639e-03\n",
      "  6.46423280e-01  1.84430589e-03 -1.67408094e-01  1.25245422e-01\n",
      " -4.10290360e-01 -6.04233205e-01  2.59512603e-01  6.60004795e-01\n",
      "  7.54667819e-01 -4.01290804e-01  1.42830729e-01 -2.71820754e-01\n",
      "  2.41763130e-01  2.02829316e-01  1.81264952e-01  3.13415617e-01\n",
      "  5.67483231e-02 -4.90105569e-01  5.28491437e-01  6.74754977e-01\n",
      "  1.68790892e-01 -3.00309569e-01  4.64224145e-02  7.93877959e-01\n",
      "  3.80928159e-01 -1.57926261e-01  4.10487920e-01 -7.79105425e-01\n",
      "  3.26375037e-01 -5.05364835e-01  2.78648794e-01 -5.15152812e-02\n",
      " -2.06727073e-01 -6.19789846e-02  1.70119196e-01  1.75728962e-01\n",
      "  1.82166159e-01  1.91009596e-01 -2.80604362e-01  7.51828909e-01\n",
      " -2.79779285e-01  5.43479174e-02  1.63487107e-01 -4.26591933e-01\n",
      "  3.49073820e-02  5.48511803e-01  1.82133615e-01 -7.72468209e-01\n",
      " -6.91080034e-01  2.96474397e-01  5.21718740e-01 -1.13743973e+00\n",
      "  1.21640831e-01  6.46901011e-01  9.60029721e-01  7.34633684e-01\n",
      " -4.16387022e-01  6.57839105e-02 -7.56771028e-01  7.20156133e-01\n",
      " -5.62682450e-01 -5.84232867e-01 -4.37523067e-01 -6.52327836e-01\n",
      " -2.28529662e-01 -1.33130953e-01  2.18872145e-01 -3.21761727e-01\n",
      "  5.96445024e-01 -1.93541065e-01 -7.17518091e-01 -4.96949494e-01\n",
      "  4.69674975e-01  3.07300568e-01 -9.71463919e-01  1.24978848e-01\n",
      " -3.79878283e-02  1.58046126e-01  2.20916927e-01 -3.51052642e-01\n",
      " -4.20417525e-02  4.26932842e-01 -5.05006909e-01 -6.01932518e-02\n",
      "  4.15865660e-01  2.97424316e-01 -9.59142208e-01  2.04490885e-01\n",
      " -4.61808413e-01 -6.80434778e-02 -1.21805735e-01 -9.98858735e-02\n",
      "  7.12632477e-01  9.01229262e-01 -4.71513212e-01 -1.08448364e-01\n",
      "  1.04184709e-01  5.65722525e-01  7.00169951e-02 -7.47732699e-01\n",
      "  3.19862366e-01 -4.48384047e-01 -6.85719103e-02 -8.86145771e-01\n",
      " -2.61659831e-01  3.28913510e-01 -9.13387895e-01  4.90397066e-01\n",
      " -2.75478899e-01  6.59660935e-01  4.45114523e-02  2.22159490e-01\n",
      " -3.27976912e-01 -6.62603080e-01  6.79866850e-01  1.10271776e+00\n",
      "  1.76696315e-01  3.38472545e-01 -4.80414808e-01 -9.22157764e-02\n",
      "  1.40376851e-01 -8.02116930e-01  1.18275851e-01 -2.01768920e-01\n",
      "  4.41746384e-01  3.56284112e-01 -2.69730568e-01 -6.08058691e-01\n",
      "  3.37202288e-02 -2.19577804e-01  5.39706945e-02 -2.84920752e-01\n",
      "  2.13828813e-02 -2.54521579e-01 -5.68793952e-01 -6.38082266e-01\n",
      "  3.67524438e-02 -5.02908111e-01 -4.53856178e-02 -5.32735050e-01\n",
      " -2.16362685e-01 -8.97385299e-01 -9.07706842e-02 -3.20823461e-01\n",
      " -5.69158971e-01  6.82889819e-01 -4.87499833e-01  8.82510692e-02\n",
      "  2.76766092e-01 -4.84780893e-02 -1.60980880e-01  1.11943170e-01\n",
      " -4.12294507e-01  8.64720941e-01  2.68672526e-01  1.05034649e-01\n",
      " -1.64567932e-01 -1.76952064e-01 -6.83492243e-01  4.35331017e-01\n",
      "  8.84453714e-01 -3.22791606e-01 -4.16629046e-01 -8.43919694e-01\n",
      " -2.09268406e-01  2.14933619e-01  5.32939672e-01  2.91088760e-01\n",
      "  3.55779320e-01 -2.93843657e-01  2.45842547e-03 -2.76758671e-01\n",
      " -7.68549293e-02  6.72333896e-01 -4.04005229e-01 -2.48854309e-02\n",
      "  1.16322666e-01  1.54603854e-01 -7.61616588e-01 -2.00053379e-01\n",
      "  6.10877633e-01 -3.88286710e-01 -5.15889883e-01 -3.92347187e-01\n",
      "  4.88173932e-01 -2.11754918e-01  8.41548085e-01 -5.72288036e-01\n",
      " -1.13509238e+00  1.79001242e-01  1.87862769e-01  6.77757442e-01\n",
      " -1.51814163e-01  2.33985901e-01  9.35107842e-02  1.69964567e-01\n",
      "  4.07521039e-01  1.44801915e+00  1.17990337e-01  4.80240971e-01\n",
      "  1.43699840e-01 -1.61629558e-01 -2.51415730e-01  3.74899745e-01\n",
      " -3.04124206e-01  6.77738637e-02  6.62834570e-02  1.84203774e-01\n",
      " -2.54750550e-01  3.72137100e-01 -4.63713050e-01  4.19889301e-01\n",
      "  9.94622260e-02  8.46610144e-02 -2.93465883e-01  1.49640232e-01\n",
      "  4.63013738e-01  2.61814315e-02  5.65891922e-01  9.40026939e-01\n",
      "  2.02721059e-01 -6.05575085e-01 -2.73544163e-01 -3.94985676e-01]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nVector for 'sleep' using CBOW:\")\n",
    "print(loaded_cbow.wv['sleep'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vector for 'sleep' using Skip-Gram:\n",
      "[ 1.07165389e-01  1.25611112e-01 -6.80771843e-02 -1.87418744e-01\n",
      "  1.10103078e-01 -2.57751346e-01 -1.62744597e-02  3.67997169e-01\n",
      " -9.56075564e-02  1.65430471e-01 -2.22483762e-02 -1.86914802e-02\n",
      " -3.31699520e-01 -1.36216208e-02  1.95318148e-01 -1.77527875e-01\n",
      " -1.96722552e-01 -4.05768782e-01  2.69870888e-02 -3.84183645e-01\n",
      "  1.58398032e-01  3.74577016e-01 -7.81415254e-02  5.91156892e-02\n",
      "  3.02761674e-01 -2.55196154e-01  9.82362330e-02  1.19395040e-01\n",
      " -2.59588480e-01 -2.26849511e-01 -2.70252734e-01  2.47389480e-01\n",
      " -2.07168199e-02  8.60508308e-02 -9.89407003e-02  1.36240646e-01\n",
      "  2.64751792e-01 -1.18487366e-02 -1.73611548e-02 -2.72767413e-02\n",
      " -8.75813887e-02 -4.02028412e-01  5.53671196e-02  9.66536850e-02\n",
      "  2.62171060e-01  1.78386956e-01 -5.96064664e-02  2.54169762e-01\n",
      "  4.10596699e-01 -1.81573015e-02 -1.36357009e-01 -1.08233258e-01\n",
      "  9.16432738e-02  1.46060139e-01 -2.13898122e-01  2.14502588e-01\n",
      " -1.38023540e-01 -6.82974979e-02  1.60141096e-01 -1.56052098e-01\n",
      " -1.17051288e-01  5.71385697e-02 -5.67308605e-01  2.46381581e-01\n",
      " -3.20461467e-02  2.67012060e-01 -1.84827000e-01  8.35826471e-02\n",
      " -3.75350952e-01 -2.26099998e-01  2.96572447e-01 -7.84616079e-03\n",
      " -2.10879650e-02 -5.41257143e-01  3.34859528e-02 -3.21931280e-02\n",
      "  5.00824042e-02 -3.74926515e-02  2.48248279e-02  1.53046042e-01\n",
      " -3.01799737e-02  8.40364024e-02  1.81662753e-01  3.07804972e-01\n",
      "  2.38218218e-01 -4.47055697e-02  1.03967682e-01 -1.59185044e-02\n",
      " -1.28414989e-01  1.99139751e-02  4.54583764e-02  1.39325529e-01\n",
      " -9.53308493e-02 -1.00084260e-01  1.51431158e-01  2.52196044e-01\n",
      "  2.26707816e-01 -2.65344143e-01 -1.96314082e-01  4.03811038e-01\n",
      "  7.18023106e-02  5.03807282e-03  2.48981461e-01 -2.93504596e-01\n",
      " -8.13648626e-02  9.35335532e-02  1.22657148e-02  1.34809628e-01\n",
      "  9.89455283e-02 -1.31953284e-02  1.46207020e-01  6.28345609e-02\n",
      "  2.44662608e-03  5.12392893e-02  1.25591329e-03  2.67339796e-01\n",
      " -2.90691048e-01 -1.63973093e-01 -1.11458581e-02 -2.46424720e-01\n",
      "  5.61970193e-03 -2.77740676e-02 -5.39817996e-02 -2.88754493e-01\n",
      " -3.92794669e-01  3.27277005e-01  6.31931573e-02 -5.21715045e-01\n",
      "  1.29239216e-01  2.45954186e-01  3.15082043e-01  2.63351500e-01\n",
      " -1.88701555e-01  2.21718326e-01 -4.06320214e-01  2.29743227e-01\n",
      " -8.18771496e-03 -1.90543768e-03 -2.72045344e-01 -9.48030800e-02\n",
      " -1.06374159e-01 -1.97443575e-01 -5.58374543e-03  2.77302042e-02\n",
      "  3.29971373e-01  2.40633905e-01 -1.02329254e-01 -6.42274320e-02\n",
      " -1.67456046e-01 -6.18768670e-02 -1.87496156e-01  2.30481267e-01\n",
      " -2.99661666e-01 -5.11875898e-02 -6.29631057e-02 -3.84343088e-01\n",
      " -1.13689080e-01  2.36518010e-01 -7.43316710e-02  2.20510960e-01\n",
      "  2.21456572e-01  1.45171314e-01 -3.68178189e-01  4.60494496e-03\n",
      " -1.60499871e-01 -1.71703994e-01  8.62549171e-02 -2.54952639e-01\n",
      "  1.90852389e-01  4.37939554e-01 -2.22902983e-01  7.21279979e-02\n",
      "  1.07277922e-01  5.78155756e-01  8.25996622e-02 -1.53596386e-01\n",
      "  3.05316985e-01 -3.19738626e-01 -2.22290725e-01 -4.18646961e-01\n",
      " -5.53680584e-02 -7.72106573e-02 -4.19123769e-01 -1.35283232e-01\n",
      "  3.55525278e-02  2.89594948e-01  1.86733708e-01  4.02830571e-01\n",
      " -2.31246158e-01 -1.73516665e-02  7.43736029e-02  5.44542134e-01\n",
      "  8.74390677e-02 -6.65855035e-02 -2.46023145e-02 -2.80895144e-01\n",
      " -1.00375041e-01 -3.16101521e-01  1.26101419e-01 -5.10228239e-02\n",
      " -9.54468735e-03 -6.84377253e-02 -1.83350950e-01 -2.48476580e-01\n",
      " -4.98248711e-02 -8.35536569e-02 -1.20877847e-01 -2.56278992e-01\n",
      "  8.20556879e-02  6.94735497e-02 -1.68171190e-02 -2.35466257e-01\n",
      " -2.04173520e-01 -2.68583894e-01 -5.78321926e-02 -1.29909888e-01\n",
      "  1.82125926e-01 -6.29567504e-01 -1.54008001e-01 -2.59499401e-01\n",
      " -1.46472650e-02  4.05274272e-01  7.21089393e-02  1.95386499e-01\n",
      " -1.00031182e-01  5.78253483e-03  8.92150998e-02  1.91143826e-01\n",
      " -3.98933262e-01  3.27841073e-01  1.29763797e-01 -5.17940558e-02\n",
      " -1.08532317e-01 -5.39156832e-02 -2.51069903e-01  4.65717651e-02\n",
      "  3.57586503e-01 -2.28509665e-01 -1.36567593e-01 -7.75377452e-01\n",
      " -1.12346977e-01  1.05931938e-01 -1.23283632e-01 -2.14712664e-01\n",
      "  4.68412548e-01 -1.67467862e-01  1.94139648e-02 -3.20951678e-02\n",
      " -5.51438071e-02  1.57110691e-01 -2.50575393e-01 -4.48452085e-02\n",
      "  2.79370010e-01  1.35704994e-01 -3.23885351e-01 -2.38110363e-01\n",
      "  2.87085325e-01  7.10426718e-02  1.32980077e-02 -3.69154215e-01\n",
      "  2.55604714e-01  2.65229698e-02  2.70367742e-01 -1.80067375e-01\n",
      " -4.08065438e-01  1.36534929e-01  4.59302105e-02  4.81813431e-01\n",
      "  1.15604021e-01  3.25720794e-02 -2.50884015e-02  2.40321472e-01\n",
      " -2.59371877e-01  3.93246979e-01  6.53261365e-03  2.44918555e-01\n",
      " -1.08212478e-01  8.90949443e-02 -5.51718473e-02 -8.08214992e-02\n",
      "  1.38384029e-01  2.28411719e-01  2.16472909e-01  2.86322296e-01\n",
      "  1.93788677e-01  1.27515495e-01 -3.14158261e-01  2.62673438e-01\n",
      " -1.09559923e-01  1.15727842e-01 -2.82195538e-01  9.86514315e-02\n",
      "  1.07634403e-01 -4.31514345e-05  6.92348853e-02  3.30561489e-01\n",
      "  8.72436762e-02 -4.66910824e-02 -9.46319662e-03  7.67861232e-02]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nVector for 'sleep' using Skip-Gram:\")\n",
    "print(loaded_skipgram.wv['sleep'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
